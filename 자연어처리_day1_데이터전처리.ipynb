{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPG8S7R40116AK8SkiPVuUj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 텍스트 전처리\n","\n","데이터를 분석하기 전에 전처리를 해줘야한다.\n","\n","#### 1. 토큰(글자, 단어, 문장, 문단)화\n","- 영어는 공백 단위로 분리만 해주면 된다.\n","- 일반적으로 단어 토큰화를 가장 많이 한다.\n","\n","    = 하나의 단어를 하나의 토큰으로 표현하는 것\n","\n","    = 한국어 언어 모델은 극안의 난이도 .. (일반적으로 공백으로 단어를 분리할 수 없다.)\n","\n","- 즉, 형태소 분석기를 통해 단어를 형태소 단위로 나누는 작업을 해야한다.\n","\n","#### 2. 불용어\n","\n","사용을 해도 필요없는 단어\n","\n","\n","##### [사용방법]\n","\n","분석하고자 하는 대상(품사에 해당하는 단어) 통일\n","\n","불용어 사전에 등록되어 있는 단어들은 모두 삭제\n","\n","#### 3. 정규표현식 (과제)\n","\n","#### 4. 인코딩\n","- 단어들을 벡터 공간에 수치화하기 위해서 사용하는 것 (행렬로 표현하기 위해서는 문자열이 아니라 수치화를 사용해야한다.)\n","\n","#### 5. Padding\n","- 각 문장에서 나온 형태소들의 길이를 동일하게 만들어주는 것"],"metadata":{"id":"uV6L5xJzsjeA"}},{"cell_type":"markdown","source":["### 자연어 처리 환경 구성 방법\n","\n","1. 텐서플로우 설치\n","\n","2. nltk 설치\n","\n","3. 설치 후\n","\n","   import nltk\n","   \n","   nltk.download() : 여러 데이터들 다운로드 받기"],"metadata":{"id":"6U_ZXswJr9LM"}},{"cell_type":"code","source":["pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65mRYxzlsBX6","executionInfo":{"status":"ok","timestamp":1690197098931,"user_tz":-540,"elapsed":5061,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"78797c35-c6a0-4432-f072-943f0fc51c33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.13)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xd3MUjAsxHV","executionInfo":{"status":"ok","timestamp":1690197106875,"user_tz":-540,"elapsed":7952,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"cd5a277e-208e-427e-b35a-7da987c77a61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}]},{"cell_type":"code","source":["pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0hHp7FwsX1k","executionInfo":{"status":"ok","timestamp":1690197115263,"user_tz":-540,"elapsed":8411,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"aadd14bc-92b6-4402-df1b-6402a5b84800"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["import konlpy\n","\n","konlpy.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"pxy0prozsp7m","executionInfo":{"status":"ok","timestamp":1690197117730,"user_tz":-540,"elapsed":5,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"653f5d5f-2ac2-4e81-9293-8e2d644c1b6a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.6.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":[],"metadata":{"id":"9snxIZkcyPWM"}},{"cell_type":"code","source":["from konlpy.tag import Okt\n","\n","okt = Okt()\n","print(okt.morphs(u'단독입찰보다 복수입찰의 경우'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIKtKKb7sp-D","executionInfo":{"status":"ok","timestamp":1690197136141,"user_tz":-540,"elapsed":14765,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"f0234af1-8754-450b-a702-ef15e0d3ae1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"]}]},{"cell_type":"code","source":["print(okt.morphs(u'아버지가방에들어가신다.'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qkPtwYVttOxc","executionInfo":{"status":"ok","timestamp":1690197137675,"user_tz":-540,"elapsed":1538,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"9a535374-9174-43b1-ddcd-cb08cdd49fd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['아버지', '가방', '에', '들어가신다', '.']\n"]}]},{"cell_type":"markdown","source":["####단어가방(Bag Of Words)\n","- 단어들이 순서와 관계없이 저장 (사전과 같은 의미)\n","\n","####코퍼스(말뭉치)\n","- 자연어처리(분석)하고자 하는 분야와 관련된 단어 집합\n","\n","  ex) 법률서비스 챗봇 ; 법률코퍼스: 기소, 벌금, ...\n","\n","\n","\n"],"metadata":{"id":"10Rm_TDbtoHp"}},{"cell_type":"code","source":["#nltk: 영어 코퍼스 토큰화 도구\n","\n","#단어 단위로 토큰화 해주는 함수\n","from nltk.tokenize import word_tokenize , WordPunctTokenizer, sent_tokenize"],"metadata":{"id":"EOeiDA2YvKTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMojbFeKvlZ5","executionInfo":{"status":"ok","timestamp":1690197141320,"user_tz":-540,"elapsed":885,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"365c0859-92f9-409b-f873-63e86b00228d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["### 영어권 단어 분리 방법\n","\n","#### word_tokenize\n","\n","- \" 가 포함되어 있는 경우 이를 기준으로 분리해주기도 한다.\n","\n","\n","#### WordPunctTokenizer\n","\n","- WordPunctTokenizer() : 객체 생성 필수\n","\n","- \" 를 따로 분류"],"metadata":{"id":"vTClPHZ5v_i2"}},{"cell_type":"code","source":["print(\"단어 토큰화 결과: \", word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wqVBa9mvUnT","executionInfo":{"status":"ok","timestamp":1690197169816,"user_tz":-540,"elapsed":387,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"3c0c9358-460f-44df-ed47-33f239de3ecb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 토큰화 결과:  ['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"]}]},{"cell_type":"code","source":["print(\"단어 토큰화 결과: \", WordPunctTokenizer().tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHj8cIFFv0qx","executionInfo":{"status":"ok","timestamp":1690197171496,"user_tz":-540,"elapsed":443,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"0666a96b-76c8-46aa-aeb2-da1e15a41204"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 토큰화 결과:  ['Tommy', \"'\", 's', 'Don', \"'\", 't', 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"]}]},{"cell_type":"markdown","source":["### 영어권 문장 분리 방법\n","\n","품사: https://happygrammer.github.io/nlp/postag-set/"],"metadata":{"id":"nnj_fRq_xlaf"}},{"cell_type":"code","source":["#마침표가 3개 존재한다.\n","data = \"Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\""],"metadata":{"id":"tRFDkc1lw9eV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#마침표와 느낌표로 분리해준다.\n","sent_tokenize(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZfPO5cTtw9g1","executionInfo":{"status":"ok","timestamp":1690197141321,"user_tz":-540,"elapsed":10,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"09f94842-4e2d-4536-c936-f32e13b9900a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Language is a thing of beauty.',\n"," 'But mastering a new language from scratch is quite a daunting prospect.',\n"," 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n"," 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["#품사 단위로 Padding\n","\n","from nltk.tag import pos_tag"],"metadata":{"id":"x2xEoYPPw9jL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtPX4bICyaqh","executionInfo":{"status":"ok","timestamp":1690197201566,"user_tz":-540,"elapsed":280,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"b6b6be2c-759d-4e3a-a7ce-0939bd397b5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["#토큰과 품사가 튜플로 출력된다.\n","print(\"단어 토큰화 결과: \", word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))\n","res = word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\")\n","pos_tag(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EV7vU084w9lR","executionInfo":{"status":"ok","timestamp":1690197203816,"user_tz":-540,"elapsed":491,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"0f884943-0fa4-441d-a526-22afe75da31a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 토큰화 결과:  ['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"]},{"output_type":"execute_result","data":{"text/plain":["[('Tommy', 'NNP'),\n"," (\"'s\", 'POS'),\n"," ('Do', 'VBP'),\n"," (\"n't\", 'RB'),\n"," ('And', 'CC'),\n"," ('that', 'IN'),\n"," ('’', 'NNP'),\n"," ('s', 'VBZ'),\n"," ('exactly', 'RB'),\n"," ('the', 'DT'),\n"," ('way', 'NN'),\n"," ('with', 'IN'),\n"," ('our', 'PRP$'),\n"," ('machines', 'NNS'),\n"," ('.', '.'),\n"," ('In', 'IN'),\n"," ('order', 'NN'),\n"," ('to', 'TO'),\n"," ('get', 'VB'),\n"," ('our', 'PRP$'),\n"," ('computer', 'NN'),\n"," ('to', 'TO'),\n"," ('understand', 'VB'),\n"," ('any', 'DT'),\n"," ('text', 'NN'),\n"," (',', ','),\n"," ('we', 'PRP'),\n"," ('need', 'VBP'),\n"," ('to', 'TO'),\n"," ('break', 'VB'),\n"," ('that', 'IN'),\n"," ('word', 'NN'),\n"," ('down', 'RP'),\n"," ('in', 'IN'),\n"," ('a', 'DT'),\n"," ('way', 'NN'),\n"," ('that', 'IN'),\n"," ('our', 'PRP$'),\n"," ('machine', 'NN'),\n"," ('can', 'MD'),\n"," ('understand', 'VB'),\n"," ('.', '.'),\n"," ('That', 'DT'),\n"," ('’', 'VBZ'),\n"," ('s', 'NN'),\n"," ('where', 'WRB'),\n"," ('the', 'DT'),\n"," ('concept', 'NN'),\n"," ('of', 'IN'),\n"," ('tokenization', 'NN'),\n"," ('in', 'IN'),\n"," ('Natural', 'NNP'),\n"," ('Language', 'NNP'),\n"," ('Processing', 'NNP'),\n"," ('(', '('),\n"," ('NLP', 'NNP'),\n"," (')', ')'),\n"," ('comes', 'VBZ'),\n"," ('in', 'IN'),\n"," ('.', '.')]"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["### KSS\n","#### 한국어 문장이 주어졌을 때 문장 단위로 분리해주는 토큰화 도구\n","\n","- 문단 사이에 의미, 관계와 같은 것들을 파악하여 문제를 해결하려고 한다.\n","\n","- 문장 단위로 토큰화하게 되면은 문장 단위로 처리하겠다는 것!!\n","\n","- 즉, 문장 사이의 관계를 분석하여 중요한 문장을 뽑아내거나 관계를 파악을 하는 작업을 수행한다."],"metadata":{"id":"77JZklzrF_s7"}},{"cell_type":"code","source":["pip install kss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYl8cm1nBj22","executionInfo":{"status":"ok","timestamp":1690197502514,"user_tz":-540,"elapsed":184839,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"2765b920-a9d3-45e3-f895-72eee00b4f8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kss\n","  Downloading kss-4.5.4.tar.gz (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting emoji==1.2.0 (from kss)\n","  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n","Collecting pecab (from kss)\n","  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.2.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.2)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n","Building wheels for collected packages: kss, pecab\n","  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kss: filename=kss-4.5.4-py3-none-any.whl size=54467 sha256=59485d1291b95e557425c1958dd699dcfd8e69992edfe9549e4111b60497b236\n","  Stored in directory: /root/.cache/pip/wheels/61/7b/ba/e620ef5d96a61cdd83bdee4c2bb4aec8a74de5d72fcbb00e80\n","  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646666 sha256=56de68f30913c71970e66d76d81b4b2360c63661680026943daf1b1984e31288\n","  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n","Successfully built kss pecab\n","Installing collected packages: emoji, pecab, kss\n","Successfully installed emoji-1.2.0 kss-4.5.4 pecab-1.0.8\n"]}]},{"cell_type":"code","source":["import kss"],"metadata":{"id":"ffMfwuRGF3ke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"날씨가 덥다. 딥러닝을 공부합니다. 여름이었다. 네?\""],"metadata":{"id":"0rqvTPWOGIDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kss.split_sentences(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2BicK5JGZmo","executionInfo":{"status":"ok","timestamp":1690174970914,"user_tz":-540,"elapsed":15,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"881ed6d2-63ad-4a51-b7bd-686bad8fa0b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n","For your information, Kss also supports mecab backend.\n","We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n","Please refer to following web sites for details:\n","- mecab: https://github.com/hyunwoongko/python-mecab-kor\n","- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['날씨가 덥다.', '딥러닝을 공부합니다.', '여름이었다.', '네?']"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["### Okt VS Kkma\n","\n","Okt가 성능이 더 좋음 !!!"],"metadata":{"id":"hnSSbMHlJuyl"}},{"cell_type":"code","source":["from konlpy.tag import Okt, Kkma"],"metadata":{"id":"jhCvcaXmIDhC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["okt = Okt()\n","kkma = Kkma()"],"metadata":{"id":"hdVQDSlzIl3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Okt : \", okt.morphs(\"NLP를 열심히 공부하고, 취업에 성공합시다\")) #형태소 단어 분석\n","print(\"Okt : \", okt.pos(\"NLP를 열심히 공부하고, 취업에 성공합시다\")) #(단어, 품사)\n","print(\"Okt : \", okt.nouns(\"NLP를 열심히 공부하고, 취업에 성공합시다\")) #명사 추출 - 제대로 안나오는 단어들도 몇 가지 존재"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2sQruBrpIxZV","executionInfo":{"status":"ok","timestamp":1690174970915,"user_tz":-540,"elapsed":12,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"03794805-7ed2-4884-99af-c29652043be0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Okt :  ['NLP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다']\n","Okt :  [('NLP', 'Alpha'), ('를', 'Noun'), ('열심히', 'Adverb'), ('공부', 'Noun'), ('하고', 'Josa'), (',', 'Punctuation'), ('취업', 'Noun'), ('에', 'Josa'), ('성공합시다', 'Adjective')]\n","Okt :  ['를', '공부', '취업']\n"]}]},{"cell_type":"code","source":["print(\"Kkma : \", kkma.morphs(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))\n","print(\"Kkma : \", kkma.pos(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))\n","print(\"Kkma : \", kkma.nouns(\"NLP를 열심히 공부하고, 취업에 성공합시다\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dx_PdqSgJNcE","executionInfo":{"status":"ok","timestamp":1690174996326,"user_tz":-540,"elapsed":25419,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"dd1cecea-da42-4ff2-dc88-1d00f00df5d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Kkma :  ['NLP', '를', '열심히', '공부', '하', '고', ',', '취업', '에', '성공', '합', '시다']\n","Kkma :  [('NLP', 'OL'), ('를', 'JKO'), ('열심히', 'MAG'), ('공부', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), (',', 'SP'), ('취업', 'NNG'), ('에', 'JKM'), ('성공', 'NNG'), ('합', 'NNG'), ('시다', 'NNG')]\n","Kkma :  ['공부', '취업', '성공', '성공합시다', '합', '시다']\n"]}]},{"cell_type":"markdown","source":["#### 단어의 빈도수가 낮거나 길이가 매우 짧은 단어는 상황에 따라 제거를 고려한다.\n","\n","#### *정규표현식을 사용해야하는 이유\n"],"metadata":{"id":"_q3VzQfMKxFJ"}},{"cell_type":"code","source":["import re #정규표현식 패키지\n","text = \"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\""],"metadata":{"id":"Y9WXfA1yKJZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어의 길이가 2글자 이하인 단어들이 제거된다.(따옴표가 들어간 단어들 또한 제거)\n","pat = re.compile(r'\\W*\\b\\w{1,2}\\b')"],"metadata":{"id":"bbSNmrHmKJkP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pat.sub('', text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"UuPCD2h_LKZ_","executionInfo":{"status":"ok","timestamp":1690198132933,"user_tz":-540,"elapsed":488,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"d80b5bfb-c25c-4294-fb04-ec8490f1d87d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tommy Don And that exactly the way with our machines order get our computer understand any text need break that word down way that our machine can understand. That where the concept tokenization Natural Language Processing (NLP) comes.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### 불용어"],"metadata":{"id":"Vf-iKQESL_MX"}},{"cell_type":"code","source":["#불용어가 들어가있는 sub package\n","import nltk\n","from nltk.corpus import stopwords"],"metadata":{"id":"b7bbvBWaL-JP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')\n","stopwords.words('english') #영어 불용어 리스트 출력"],"metadata":{"id":"125E7gnKMVRY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690198139967,"user_tz":-540,"elapsed":5,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"5fb2d6ef-08e2-46a2-e83f-cc30dcc63f9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," \"you're\",\n"," \"you've\",\n"," \"you'll\",\n"," \"you'd\",\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," \"she's\",\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," \"it's\",\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," \"that'll\",\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," \"don't\",\n"," 'should',\n"," \"should've\",\n"," 'now',\n"," 'd',\n"," 'll',\n"," 'm',\n"," 'o',\n"," 're',\n"," 've',\n"," 'y',\n"," 'ain',\n"," 'aren',\n"," \"aren't\",\n"," 'couldn',\n"," \"couldn't\",\n"," 'didn',\n"," \"didn't\",\n"," 'doesn',\n"," \"doesn't\",\n"," 'hadn',\n"," \"hadn't\",\n"," 'hasn',\n"," \"hasn't\",\n"," 'haven',\n"," \"haven't\",\n"," 'isn',\n"," \"isn't\",\n"," 'ma',\n"," 'mightn',\n"," \"mightn't\",\n"," 'mustn',\n"," \"mustn't\",\n"," 'needn',\n"," \"needn't\",\n"," 'shan',\n"," \"shan't\",\n"," 'shouldn',\n"," \"shouldn't\",\n"," 'wasn',\n"," \"wasn't\",\n"," 'weren',\n"," \"weren't\",\n"," 'won',\n"," \"won't\",\n"," 'wouldn',\n"," \"wouldn't\"]"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["len(set(stopwords.words('english'))) #179"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puqNDmQlqKPm","executionInfo":{"status":"ok","timestamp":1690198142708,"user_tz":-540,"elapsed":295,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"4d258ad8-af00-4d5c-99ce-2afd837dcef9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["179"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["len(stopwords.words('english'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlWroXd8qfOE","executionInfo":{"status":"ok","timestamp":1690198224127,"user_tz":-540,"elapsed":4,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"d54101c0-332b-46a5-d638-1c019ab5bf1a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["179"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["#중복되는 불용어를 제외해주기 위해 집합 사용, 이를 변수에 저장\n","sw = set(stopwords.words('english'))"],"metadata":{"id":"oDH2L-CDMuoh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 불용어들을 제거해주는 작업"],"metadata":{"id":"YOvoxj00NLiV"}},{"cell_type":"code","source":["wt = word_tokenize(text)\n","res = []\n","\n","for w in wt:\n","  if w not in sw: #불용어가 text에 있을 때\n","    res.append(w) #res 리스트에 추가한다."],"metadata":{"id":"0LYOQLDXNM4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#불용어 제외하기 전\n","print(wt)\n","print(len(wt))"],"metadata":{"id":"t5KCHVfZNM7a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690200877421,"user_tz":-540,"elapsed":414,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"906029f6-9a50-4f1d-f7f1-b3d8e47570d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'a', 'key', '(', 'and', 'mandatory', ')', 'aspect', 'of', 'working', 'with', 'text', 'data', 'We', '’', 'll', 'discuss', 'the', 'various', 'nuances', 'of', 'tokenization', ',', 'including', 'how', 'to', 'handle', 'Out-of-Vocabulary', 'words', '(', 'OOV', ')', 'Language', 'is', 'a', 'thing', 'of', 'beauty', '.', 'But', 'mastering', 'a', 'new', 'language', 'from', 'scratch', 'is', 'quite', 'a', 'daunting', 'prospect', '.', 'If', 'you', '’', 've', 'ever', 'picked', 'up', 'a', 'language', 'that', 'wasn', '’', 't', 'your', 'mother', 'tongue', ',', 'you', '’', 'll', 'relate', 'to', 'this', '!', 'There', 'are', 'so', 'many', 'layers', 'to', 'peel', 'off', 'and', 'syntaxes', 'to', 'consider', '–', 'it', '’', 's', 'quite', 'a', 'challenge', '.', 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.', 'Simply', 'put', ',', 'we', 'can', '’', 't', 'work', 'with', 'text', 'data', 'if', 'we', 'don', '’', 't', 'perform', 'tokenization', '.', 'Yes', ',', 'it', '’', 's', 'really', 'that', 'important', '!']\n","181\n"]}]},{"cell_type":"code","source":["#불용어 제외 후\n","print(res)\n","print(len(res))"],"metadata":{"id":"L0569hCaOD-m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690200880182,"user_tz":-540,"elapsed":418,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"363c734d-d44c-4339-a3ee-da1f258f285d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'key', '(', 'mandatory', ')', 'aspect', 'working', 'text', 'data', 'We', '’', 'discuss', 'various', 'nuances', 'tokenization', ',', 'including', 'handle', 'Out-of-Vocabulary', 'words', '(', 'OOV', ')', 'Language', 'thing', 'beauty', '.', 'But', 'mastering', 'new', 'language', 'scratch', 'quite', 'daunting', 'prospect', '.', 'If', '’', 'ever', 'picked', 'language', '’', 'mother', 'tongue', ',', '’', 'relate', '!', 'There', 'many', 'layers', 'peel', 'syntaxes', 'consider', '–', '’', 'quite', 'challenge', '.', 'And', '’', 'exactly', 'way', 'machines', '.', 'In', 'order', 'get', 'computer', 'understand', 'text', ',', 'need', 'break', 'word', 'way', 'machine', 'understand', '.', 'That', '’', 'concept', 'tokenization', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', '.', 'Simply', 'put', ',', '’', 'work', 'text', 'data', '’', 'perform', 'tokenization', '.', 'Yes', ',', '’', 'really', 'important', '!']\n","108\n"]}]},{"cell_type":"code","source":["#한국어 변환\n","text = \"NLP를 열심히 공부하고, 취업에 성공합시다.\"\n","sw = \"를 에 고 라고 다\""],"metadata":{"id":"cx_iAC9KOwJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#sw를 공백에 따라 나누어서 리스트에 넣어준다.\n","sw = sw.split(\" \")\n","sw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YlMndguyO3GI","executionInfo":{"status":"ok","timestamp":1690200884230,"user_tz":-540,"elapsed":383,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"dd455ec2-8710-41ac-b539-7848191e5e8a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['를', '에', '고', '라고', '다']"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["#text를 okt를 사용하여 형태소로 나누어준다.\n","wt = okt.morphs(text)\n","#wt에 담겨있는 형태소로 나누어진 단어들이 sw(stop word)에 없다면, 불용어가 아니기 때문에 사용을 해야하는 단어이며,\n","#이 단어들을 list 형태로 출력한다.\n","res = [w for w in wt if not w in sw]"],"metadata":{"id":"Sfb4i36OO3IL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PST9IzdPDPc","executionInfo":{"status":"ok","timestamp":1690200888963,"user_tz":-540,"elapsed":292,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"75a693b4-2367-4383-e580-6607bd5361e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['NLP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다', '.']"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["res #불용어에 있던 단어들이 제외되는 것을 알 수 있다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBA6BYJKPDRm","executionInfo":{"status":"ok","timestamp":1690200891245,"user_tz":-540,"elapsed":315,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"bf7e2b7a-e1b8-435e-c85a-65f76407a29f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['NLP', '열심히', '공부', '하고', ',', '취업', '성공합시다', '.']"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["### 인코딩\n","\n","One - Hot - Encoding\n","\n","단어 - 정수화 (index)\n","\n","파이썬 딕셔너리를 사용하는 방법\n","\n","ex) good hi hello - 0, 1, 2 = 100 / 010 / 001"],"metadata":{"id":"AlItmK8BT_ZT"}},{"cell_type":"code","source":["text = \"\"\"\n","Tokenization is a key (and mandatory) aspect of working with text data\n","We’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\n","Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\n","And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\n","Simply put, we can’t work with text data if we don’t perform tokenization. Yes, it’s really that important!\n","\"\"\""],"metadata":{"id":"ZFwqrvVqUyrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#불용어를 집합으로 만들어준다ㅣ\n","sw = set(stopwords.words('english'))"],"metadata":{"id":"grHA4dcpg3do"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#text를 문장 단위로 토큰화해준다.\n","sents = sent_tokenize(text)"],"metadata":{"id":"-MwYZf5rUyt2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#토큰화한 뒤 리스트로 생성\n","sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ab2MyL-lVN32","executionInfo":{"status":"ok","timestamp":1690200902488,"user_tz":-540,"elapsed":414,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"253eb117-b737-4060-9746-697ae64fd9f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\nTokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.',\n"," 'But mastering a new language from scratch is quite a daunting prospect.',\n"," 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n"," 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.',\n"," 'And that’s exactly the way with our machines.',\n"," 'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.',\n"," 'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.',\n"," 'Simply put, we can’t work with text data if we don’t perform tokenization.',\n"," 'Yes, it’s really that important!']"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["#문장의 개수\n","len(sents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rkEjpU3KVWL_","executionInfo":{"status":"ok","timestamp":1690200905168,"user_tz":-540,"elapsed":312,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"a82f2afc-389a-474e-e38c-d18e76831c87"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["vocab = {}\n","pre_sents = []\n","\n","for s in sents :\n","  # 문장에 있는 단어들을 토큰화해준다.\n","  wt = word_tokenize(s)\n","\n","  #불용어가 아니고 길이가 2보다 큰 단어들을 리스트에 추가한다.\n","  res = []\n","\n","  for w in wt:\n","    w = w.lower() #영어 단어인 경우, 대문자와 소문자의 구분이 가능 - 대소문자를 통일해주어야 한다.: 소문자로 통일\n","    #불용어가 아닌 경우\n","    if w not in sw:\n","      if len(w) > 2: #길이가 2 이상인 경우에는 res 리스트에 추가해준다.\n","        res.append(w)\n","        if w not in vocab: #딕셔너에 값이 없으면\n","          vocab[w] = 0 #단어를 vocab에 w를 key로 넣어주고 0을 value로 넣어준다. (빈도 계산)\n","        vocab[w] += 1 #딕셔너리에 값이 있으면 1만큼 증가한다.\n","  pre_sents.append(res) #한 문장에서 조건에 따라 나누어진 문장들의 리스트를 pre_sents 리스트에 각각 넣어준다.\n","\n","print(pre_sents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_JTxQwdAVWNz","executionInfo":{"status":"ok","timestamp":1690201066295,"user_tz":-540,"elapsed":406,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"71dbdefb-a4a3-4132-a502-b9e36e53a36f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['tokenization', 'key', 'mandatory', 'aspect', 'working', 'text', 'data', 'discuss', 'various', 'nuances', 'tokenization', 'including', 'handle', 'out-of-vocabulary', 'words', 'oov', 'language', 'thing', 'beauty'], ['mastering', 'new', 'language', 'scratch', 'quite', 'daunting', 'prospect'], ['ever', 'picked', 'language', 'mother', 'tongue', 'relate'], ['many', 'layers', 'peel', 'syntaxes', 'consider', 'quite', 'challenge'], ['exactly', 'way', 'machines'], ['order', 'get', 'computer', 'understand', 'text', 'need', 'break', 'word', 'way', 'machine', 'understand'], ['concept', 'tokenization', 'natural', 'language', 'processing', 'nlp', 'comes'], ['simply', 'put', 'work', 'text', 'data', 'perform', 'tokenization'], ['yes', 'really', 'important']]\n"]}]},{"cell_type":"code","source":["vocab #단어의 빈도수 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACKfZbHiVWPc","executionInfo":{"status":"ok","timestamp":1690178941364,"user_tz":-540,"elapsed":307,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"27e2184b-e012-4c57-fcd9-335c63aca536"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokenization': 4,\n"," 'key': 1,\n"," 'mandatory': 1,\n"," 'aspect': 1,\n"," 'working': 1,\n"," 'text': 3,\n"," 'data': 2,\n"," 'discuss': 1,\n"," 'various': 1,\n"," 'nuances': 1,\n"," 'including': 1,\n"," 'handle': 1,\n"," 'out-of-vocabulary': 1,\n"," 'words': 1,\n"," 'oov': 1,\n"," 'language': 4,\n"," 'thing': 1,\n"," 'beauty': 1,\n"," 'mastering': 1,\n"," 'new': 1,\n"," 'scratch': 1,\n"," 'quite': 2,\n"," 'daunting': 1,\n"," 'prospect': 1,\n"," 'ever': 1,\n"," 'picked': 1,\n"," 'mother': 1,\n"," 'tongue': 1,\n"," 'relate': 1,\n"," 'many': 1,\n"," 'layers': 1,\n"," 'peel': 1,\n"," 'syntaxes': 1,\n"," 'consider': 1,\n"," 'challenge': 1,\n"," 'exactly': 1,\n"," 'way': 2,\n"," 'machines': 1,\n"," 'order': 1,\n"," 'get': 1,\n"," 'computer': 1,\n"," 'understand': 2,\n"," 'need': 1,\n"," 'break': 1,\n"," 'word': 1,\n"," 'machine': 1,\n"," 'concept': 1,\n"," 'natural': 1,\n"," 'processing': 1,\n"," 'nlp': 1,\n"," 'comes': 1,\n"," 'simply': 1,\n"," 'put': 1,\n"," 'work': 1,\n"," 'perform': 1,\n"," 'yes': 1,\n"," 'really': 1,\n"," 'important': 1}"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["vocab['understand'] # 'understand'의 빈도 수"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sjsv5YshVWRP","executionInfo":{"status":"ok","timestamp":1690201093073,"user_tz":-540,"elapsed":313,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"70c91b27-6a75-401a-9888-2baff73c36d5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["#빈도수가 많은 순서대로 출력\n","#리스트 구조에 튜플로 되어 있음\n","\n","vocab.items() #(딕셔너리가 튜플로 출력된다.)\n","\n","#정렬 기준은 key, x에는 ('tokenization', 4) ... 전달된다. (1번 인덱스가 빈도수) / reverse = 내림차순\n","vs = sorted(vocab.items(), key=lambda x:x[1], reverse = True )\n","print(vs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zr1m8O7sVWUv","executionInfo":{"status":"ok","timestamp":1690201105786,"user_tz":-540,"elapsed":508,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"4b051c6b-f194-49c6-8d53-67fc373106bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('tokenization', 4), ('language', 4), ('text', 3), ('data', 2), ('quite', 2), ('way', 2), ('understand', 2), ('key', 1), ('mandatory', 1), ('aspect', 1), ('working', 1), ('discuss', 1), ('various', 1), ('nuances', 1), ('including', 1), ('handle', 1), ('out-of-vocabulary', 1), ('words', 1), ('oov', 1), ('thing', 1), ('beauty', 1), ('mastering', 1), ('new', 1), ('scratch', 1), ('daunting', 1), ('prospect', 1), ('ever', 1), ('picked', 1), ('mother', 1), ('tongue', 1), ('relate', 1), ('many', 1), ('layers', 1), ('peel', 1), ('syntaxes', 1), ('consider', 1), ('challenge', 1), ('exactly', 1), ('machines', 1), ('order', 1), ('get', 1), ('computer', 1), ('need', 1), ('break', 1), ('word', 1), ('machine', 1), ('concept', 1), ('natural', 1), ('processing', 1), ('nlp', 1), ('comes', 1), ('simply', 1), ('put', 1), ('work', 1), ('perform', 1), ('yes', 1), ('really', 1), ('important', 1)]\n"]}]},{"cell_type":"code","source":["#빈도수가 2 이상인 단어들에 대해서만 index를 부여한다.\n","word_index = {}\n","a = 0\n","for w, f in vs: #(key, value)가 딕셔너리에 있을 때\n","  if f>=2: #빈도수가 2보다 크면,\n","    a += 1 #value에 인덱스를 더해준다.\n","    word_index[w] = a # value 값에 index를 추가해준다.\n","word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XgVBRbCVWW8","executionInfo":{"status":"ok","timestamp":1690201253231,"user_tz":-540,"elapsed":308,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"217d597b-fb91-4ca6-d6f7-7426e3c190d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokenization': 1,\n"," 'language': 2,\n"," 'text': 3,\n"," 'data': 4,\n"," 'quite': 5,\n"," 'way': 6,\n"," 'understand': 7}"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EiZOPkAecHNd","executionInfo":{"status":"ok","timestamp":1690201255400,"user_tz":-540,"elapsed":327,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"c694b2e0-a589-4ec1-ae1c-4f5c2de53ce3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\nTokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.',\n"," 'But mastering a new language from scratch is quite a daunting prospect.',\n"," 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n"," 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.',\n"," 'And that’s exactly the way with our machines.',\n"," 'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.',\n"," 'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.',\n"," 'Simply put, we can’t work with text data if we don’t perform tokenization.',\n"," 'Yes, it’s really that important!']"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["데이터가 많으면 좋지만, 데이터의 품질도 중요하다."],"metadata":{"id":"ShMOoLMahfaV"}},{"cell_type":"code","source":["pre_sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TE6SU5Abcpvc","executionInfo":{"status":"ok","timestamp":1690201257645,"user_tz":-540,"elapsed":322,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"8a6019f1-06a8-4af2-d151-6da65cd9e70b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['tokenization',\n","  'key',\n","  'mandatory',\n","  'aspect',\n","  'working',\n","  'text',\n","  'data',\n","  'discuss',\n","  'various',\n","  'nuances',\n","  'tokenization',\n","  'including',\n","  'handle',\n","  'out-of-vocabulary',\n","  'words',\n","  'oov',\n","  'language',\n","  'thing',\n","  'beauty'],\n"," ['mastering', 'new', 'language', 'scratch', 'quite', 'daunting', 'prospect'],\n"," ['ever', 'picked', 'language', 'mother', 'tongue', 'relate'],\n"," ['many', 'layers', 'peel', 'syntaxes', 'consider', 'quite', 'challenge'],\n"," ['exactly', 'way', 'machines'],\n"," ['order',\n","  'get',\n","  'computer',\n","  'understand',\n","  'text',\n","  'need',\n","  'break',\n","  'word',\n","  'way',\n","  'machine',\n","  'understand'],\n"," ['concept',\n","  'tokenization',\n","  'natural',\n","  'language',\n","  'processing',\n","  'nlp',\n","  'comes'],\n"," ['simply', 'put', 'work', 'text', 'data', 'perform', 'tokenization'],\n"," ['yes', 'really', 'important']]"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KPiL5OAeid7v","executionInfo":{"status":"ok","timestamp":1690201304993,"user_tz":-540,"elapsed":351,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"d36b77e2-2dcc-40b0-f487-0a50eb96ecc9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokenization': 1,\n"," 'language': 2,\n"," 'text': 3,\n"," 'data': 4,\n"," 'quite': 5,\n"," 'way': 6,\n"," 'understand': 7}"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["len(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSDXC8it2QNK","executionInfo":{"status":"ok","timestamp":1690201307108,"user_tz":-540,"elapsed":308,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"39f1589e-1117-4838-f1b7-7c04ed81259d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["#이걸 해주는 이유는 각 문장들의 리스트별 길이가 다르기 때문에\n","word_index['OOV'] = len(word_index) + 1 #Out Of Vocab word_index['OOV'] = 8 (key = 'OOV', value = 8)"],"metadata":{"id":"iamvRFDgjO0G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pre_sents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvjCTm6g3Aeo","executionInfo":{"status":"ok","timestamp":1690201498584,"user_tz":-540,"elapsed":629,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"018258bb-a1ff-4c25-d0fb-f9c791dc3bdb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['tokenization', 'key', 'mandatory', 'aspect', 'working', 'text', 'data', 'discuss', 'various', 'nuances', 'tokenization', 'including', 'handle', 'out-of-vocabulary', 'words', 'oov', 'language', 'thing', 'beauty'], ['mastering', 'new', 'language', 'scratch', 'quite', 'daunting', 'prospect'], ['ever', 'picked', 'language', 'mother', 'tongue', 'relate'], ['many', 'layers', 'peel', 'syntaxes', 'consider', 'quite', 'challenge'], ['exactly', 'way', 'machines'], ['order', 'get', 'computer', 'understand', 'text', 'need', 'break', 'word', 'way', 'machine', 'understand'], ['concept', 'tokenization', 'natural', 'language', 'processing', 'nlp', 'comes'], ['simply', 'put', 'work', 'text', 'data', 'perform', 'tokenization'], ['yes', 'really', 'important']]\n"]}]},{"cell_type":"code","source":["enc_sents = [] #인코딩 문장\n","for s in pre_sents:\n","  enc_sent = []\n","  for w in s:\n","    try:\n","      enc_sent.append(word_index[w])\n","    except KeyError:\n","      enc_sent.append(word_index['OOV']) #Out Of Vocabulary (만약에 키 에러가 뜨면 8을 추가한다.)\n","  enc_sents.append(enc_sent)\n","\n","enc_sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQ7aGNW8cpxu","executionInfo":{"status":"ok","timestamp":1690201441093,"user_tz":-540,"elapsed":368,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"cbeda893-539c-4a19-8265-8fb16bbdb00a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 8, 8, 8, 8, 3, 4, 8, 8, 8, 1, 8, 8, 8, 8, 8, 2, 8, 8],\n"," [8, 8, 2, 8, 5, 8, 8],\n"," [8, 8, 2, 8, 8, 8],\n"," [8, 8, 8, 8, 8, 5, 8],\n"," [8, 6, 8],\n"," [8, 8, 8, 7, 3, 8, 8, 8, 6, 8, 7],\n"," [8, 1, 8, 2, 8, 8, 8],\n"," [8, 8, 8, 3, 4, 8, 1],\n"," [8, 8, 8]]"]},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["### 파이썬 도구\n","\n","1. 데이터 수집 (크롤링) 도구: selenium, beautiful soup, ...\n","2. 시각화 도구: matplotlib, tableau, seaborn, folium\n","3. 데이터 분석 도구: numpy, pandas\n","4. 머신러닝 도구: scikit-learn\n","5. 딥러닝 프레임웍: tesorflow(keras), pytorch, ..."],"metadata":{"id":"Q_SEczMVk1UA"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer"],"metadata":{"id":"2SaZE6UQlxp9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pre_sents\n","tok = Tokenizer()\n","tok.fit_on_texts(pre_sents) #단어에 인덱스 할당"],"metadata":{"id":"vupPlQkqmSCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.word_index #인덱스 딕셔너리\n","tok.word_counts #빈도 딕셔너리"],"metadata":{"id":"TRK-_1JNmSFo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690205035391,"user_tz":-540,"elapsed":347,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"6882357f-161f-4cfc-a2fe-2e3853c1dc99"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict()"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","source":["### Padding 사용하기\n","\n","1. 단어(PAD)에 대해 0으로 설정하여 길이를 맞춘다."],"metadata":{"id":"1mYi0YUEpb9G"}},{"cell_type":"code","source":["#Padding 사용방법\n","\n","pre_sentences = [['driver', 'person'], ['driver', 'good', 'person'], ['driver', 'huge', 'person'], ['knew', 'bad'], ['bad', 'kept', 'huge', 'bad'], ['huge', 'bad']]"],"metadata":{"id":"FWXk9WKXnVE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1. 각각의 단어들에 번호를 부여한다.\n","\n","#단어 정수 인코딩\n","tok = Tokenizer()\n","tok.fit_on_texts(pre_sentences) #인코딩하고자하는 단어들이 저장된 이중배열리스트를 넣어준다. - 번호를 부여하는 작업\n","tok.word_index"],"metadata":{"id":"Hvsgitc5nVG5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690205270703,"user_tz":-540,"elapsed":469,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"8f4afda7-22db-444e-dddd-3dba1d67ffa8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bad': 1,\n"," 'driver': 2,\n"," 'person': 3,\n"," 'huge': 4,\n"," 'good': 5,\n"," 'knew': 6,\n"," 'kept': 7}"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["encoded = tok.texts_to_sequences(pre_sentences) #단어들이 저장된 인덱스들을 출력해준다.\n","encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qud3uGOInVJB","executionInfo":{"status":"ok","timestamp":1690205272824,"user_tz":-540,"elapsed":377,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"6f572de3-037d-40d2-9c0e-68bdec801abf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["#LSTM 구조를 사용하기 위해서는 배열의 길이가 같아야 한다.\n","#배열은 4가 가장 길기 때문에 이것을 기준으로 단어의 길이를 늘려줘야 한다.\n","\n","maxlen = max(len(i) for i in encoded) #최대길이가 나온다. = 4 (각 리스트별 요소의 개수를 의미한다.)\n","maxlen"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikvUaC1LnVLE","executionInfo":{"status":"ok","timestamp":1690205273916,"user_tz":-540,"elapsed":7,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"cd50de7b-a320-4673-b18a-c9e001ad73f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["for s in encoded:\n","  while len(s) < maxlen: #maxlen보다 길이가 작은 리스트들은 길이가 4가 될 때까지 0을 추가한다.\n","    s.append(0)"],"metadata":{"id":"Sdw7U969nVNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4XR2j8GnVQg","executionInfo":{"status":"ok","timestamp":1690181413534,"user_tz":-540,"elapsed":356,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"c6072363-debe-4cdf-e8d3-e792bba2f009"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 1, 0, 0]"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XmBoqQ1qeuw","executionInfo":{"status":"ok","timestamp":1690205277103,"user_tz":-540,"elapsed":6,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"165ee4f7-8b77-45b8-e83b-e005cb8c0141"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 3, 0, 0],\n"," [2, 5, 3, 0],\n"," [2, 4, 3, 0],\n"," [6, 1, 0, 0],\n"," [1, 7, 4, 1],\n"," [4, 1, 0, 0]]"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"23IOGOqjqmQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.array(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFpxtGPtqmT7","executionInfo":{"status":"ok","timestamp":1690205282107,"user_tz":-540,"elapsed":303,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"acb005e7-caf9-44fb-99b0-6dbc8a8e4296"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 3, 0, 0],\n","       [2, 5, 3, 0],\n","       [2, 4, 3, 0],\n","       [6, 1, 0, 0],\n","       [1, 7, 4, 1],\n","       [4, 1, 0, 0]])"]},"metadata":{},"execution_count":80}]},{"cell_type":"markdown","source":["#### 케라스 도구를 이용한 패딩"],"metadata":{"id":"hRQa_ST3q4bh"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"VjEhO_v3qrNB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded = tok.texts_to_sequences(pre_sentences)\n","encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kp0bQ8YnrLEe","executionInfo":{"status":"ok","timestamp":1690205287809,"user_tz":-540,"elapsed":4,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"861c763e-c198-4a82-825b-fd4ce3937ef7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["#padded = pad_sequences(encoded, padding = 'post', maxlen = 10)\n","padded = pad_sequences(encoded, padding = 'post') #post는 0을 뒤로 보내주는 역할을 한다. (pre가 기본값)\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggIxyACRrLJV","executionInfo":{"status":"ok","timestamp":1690205290295,"user_tz":-540,"elapsed":559,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"2c49ce99-ef25-4988-d461-e29beb80adb4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 3, 0, 0],\n","       [2, 5, 3, 0],\n","       [2, 4, 3, 0],\n","       [6, 1, 0, 0],\n","       [1, 7, 4, 1],\n","       [4, 1, 0, 0]], dtype=int32)"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["padded = pad_sequences(encoded, maxlen = 2, truncating = 'post') #뒷부분만 출력된다.\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NPqaB3shrc2c","executionInfo":{"status":"ok","timestamp":1690205291955,"user_tz":-540,"elapsed":326,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"52b288cc-d67e-4780-f5ca-f1195248aa63"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 3],\n","       [2, 5],\n","       [2, 4],\n","       [6, 1],\n","       [1, 7],\n","       [4, 1]], dtype=int32)"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["padded = pad_sequences(encoded, maxlen = 2, truncating = 'pre')\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbxj5TPHrc5z","executionInfo":{"status":"ok","timestamp":1690205293198,"user_tz":-540,"elapsed":6,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"d8c3bf62-854b-4574-9112-60ade325e671"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 3],\n","       [5, 3],\n","       [4, 3],\n","       [6, 1],\n","       [4, 1],\n","       [4, 1]], dtype=int32)"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","source":["### 원핫인코딩\n","\n","1. 단어 종류 : 10개\n","→ 인코딩 → 머신러닝/딥러닝 언어모델링 (분류/생성/이해 ...)\n","\n","2. 0~9번까지 번호 부여\n","\n","   (0: sky, 1: computer, 2: pencil, ...)\n","\n","   sky : 1000000000\n","   computer : 0100000000\n","   pencil : 0010000000\n","\n","\n","#### 단점:\n","\n","단어 종류가 10만개\n","\n","원핫인코딩하면 각 단어는 10만차원 벡터공간에 임베딩\n","\n","sky : 100000......00000 (메모리가 터질 수도 있다 .. ) → 원핫벡터\n","\n","\n","#### 해결방법:\n","\n","차원 축소 → 5차원 벡터 공간 임베딩 (밀집벡터)\n","\n","차원 축소: 신경망 구조를 가지고 있다."],"metadata":{"id":"xGDskTGdwbJ9"}},{"cell_type":"code","source":["tokens = okt.morphs(\"자연어처리 공부를 합니다.\")\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBHd_yumu_g5","executionInfo":{"status":"ok","timestamp":1690205298396,"user_tz":-540,"elapsed":425,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"3de51733-b5cd-4443-8c0a-2264d8f946db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['자연어', '처리', '공부', '를', '합니다', '.']"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":["#원핫인코딩\n","#코퍼스 (수집한 문서 전체)\n","text = \"점심 메뉴로 소고기볶음밥 먹었습니다. 소고기볶음밥 너무 맛있어요. 또 먹을래요.\""],"metadata":{"id":"YGrCa9duymXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok = Tokenizer()\n","tok.fit_on_texts([text])"],"metadata":{"id":"_2BmBV9vzKY3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PcN9CQNozKQG","executionInfo":{"status":"ok","timestamp":1690205303097,"user_tz":-540,"elapsed":300,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"72567dd5-229f-4aea-d2cb-cd071901a3d4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'소고기볶음밥': 1,\n"," '점심': 2,\n"," '메뉴로': 3,\n"," '먹었습니다': 4,\n"," '너무': 5,\n"," '맛있어요': 6,\n"," '또': 7,\n"," '먹을래요': 8}"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["test = \"내일 메뉴로 소고기볶음밥 또 나왔으면 좋겠다.\""],"metadata":{"id":"pOk-wT9UzKMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok.texts_to_sequences([test])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOeIMroNFYjx","executionInfo":{"status":"ok","timestamp":1690205308127,"user_tz":-540,"elapsed":502,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"27bd48af-9110-4b29-9f97-774f5462a22f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[3, 1, 7]]"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["encoded = tok.texts_to_sequences([test])[0]\n","encoded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFyyvlelzp5V","executionInfo":{"status":"ok","timestamp":1690205313900,"user_tz":-540,"elapsed":306,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"e287f48a-e3d5-49fc-819b-2d245cbb00dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3, 1, 7]"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"SM31iGr1zp7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["o_v = to_categorical(encoded) #전체 단어 집합의 길이는 8이다. 여기서 보면 '메뉴로'가 인덱스 3이니까 0, 0, 0, 1, 0, 0, 0, 0으로 출력된다.\n","o_v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1o60-ZEzp8v","executionInfo":{"status":"ok","timestamp":1690205329724,"user_tz":-540,"elapsed":446,"user":{"displayName":"윤아현","userId":"10104428921125544543"}},"outputId":"db71ceeb-c1c5-47a4-f279-d0f5f99410a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 1., 0., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"]},"metadata":{},"execution_count":97}]}]}