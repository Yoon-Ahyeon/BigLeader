{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색하신 키워드 인공지능 (으)로 총 7,472 건의 학위논문이 검색되었습니다\n",
      "15 건의 데이터를 수집하기 위해 2 페이지의 게시물을 조회합니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 페이지 내용 수집 시작합니다 =======================\n",
      "\n",
      "\n",
      "1 번째 정보를 추출하고 있습니다============\n",
      "1.번호 : 1\n",
      "2.제목 : 인공지능으로 성공하기 : 인공지능을 작동시켜 사업에 활용하는 방법\n",
      "3.작성자 : Krunic, Veljko\n",
      "4.소속기관 : 시그마프레스\n",
      "5.발표년도 : 2021\n",
      "6.학위여부 : 학위가 없습니다\n",
      "7.초록내용 : 초록이 없습니다\n",
      "8.논문 URL 주소: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=d7345961987b50bf&control_no=152968f2724112b3ffe0bdc3ef48d419&keyword=인공지능\n",
      "\n",
      "\n",
      "2 번째 정보를 추출하고 있습니다============\n",
      "1.번호 : 2\n",
      "2.제목 : 가르치는 인공지능은 가능한가? ‘장치의 교육학’을 위한 시론 : 16개의 물음의 기록들\n",
      "3.작성자 : 임완철\n",
      "4.소속기관 : 새물결\n",
      "5.발표년도 : 2020\n",
      "6.학위여부 : 학위가 없습니다\n",
      "7.초록내용 : 초록이 없습니다\n",
      "8.논문 URL 주소: http://www.riss.kr/search/detail/DetailView.do?p_mat_type=d7345961987b50bf&control_no=ae16f0d4a8545c7bffe0bdc3ef48d419&keyword=인공지능\n",
      "\n",
      "\n",
      "3 번째 정보를 추출하고 있습니다============\n",
      "1.번호 : 3\n",
      "2.제목 : 교양으로서의 인공지능 : 비즈니스 리더를 위한 AI 활용법\n",
      "3.작성자 : 이상진\n",
      "4.소속기관 : 시크릿하우스\n",
      "5.발표년도 : 2020\n",
      "6.학위여부 : 학위가 없습니다\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=114.0.5735.199)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x0045A813+48355]\n\t(No symbol) [0x003EC4B1]\n\t(No symbol) [0x002F5358]\n\t(No symbol) [0x002DD293]\n\t(No symbol) [0x0033E37B]\n\t(No symbol) [0x0034C473]\n\t(No symbol) [0x0033A536]\n\t(No symbol) [0x003182DC]\n\t(No symbol) [0x003193DD]\n\tGetHandleVerifier [0x006BAABD+2539405]\n\tGetHandleVerifier [0x006FA78F+2800735]\n\tGetHandleVerifier [0x006F456C+2775612]\n\tGetHandleVerifier [0x004E51E0+616112]\n\t(No symbol) [0x003F5F8C]\n\t(No symbol) [0x003F2328]\n\t(No symbol) [0x003F240B]\n\t(No symbol) [0x003E4FF7]\n\tBaseThreadInitThunk [0x762C7D59+25]\n\tRtlInitializeExceptionChain [0x77B9B74B+107]\n\tRtlClearBits [0x77B9B6CF+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m full_url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttp://www.riss.kr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39murl_1\n\u001b[0;32m    129\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m driver\u001b[39m.\u001b[39;49mget(full_url)\n\u001b[0;32m    132\u001b[0m soup_1 \u001b[39m=\u001b[39m BeautifulSoup(driver\u001b[39m.\u001b[39mpage_source, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)  \n\u001b[0;32m    133\u001b[0m \u001b[39mtry\u001b[39;00m :\n",
      "File \u001b[1;32mc:\\Users\\ahyeo\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:355\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[1;32mc:\\Users\\ahyeo\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:346\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    344\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    347\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    348\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\ahyeo\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=114.0.5735.199)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x0045A813+48355]\n\t(No symbol) [0x003EC4B1]\n\t(No symbol) [0x002F5358]\n\t(No symbol) [0x002DD293]\n\t(No symbol) [0x0033E37B]\n\t(No symbol) [0x0034C473]\n\t(No symbol) [0x0033A536]\n\t(No symbol) [0x003182DC]\n\t(No symbol) [0x003193DD]\n\tGetHandleVerifier [0x006BAABD+2539405]\n\tGetHandleVerifier [0x006FA78F+2800735]\n\tGetHandleVerifier [0x006F456C+2775612]\n\tGetHandleVerifier [0x004E51E0+616112]\n\t(No symbol) [0x003F5F8C]\n\t(No symbol) [0x003F2328]\n\t(No symbol) [0x003F240B]\n\t(No symbol) [0x003E4FF7]\n\tBaseThreadInitThunk [0x762C7D59+25]\n\tRtlInitializeExceptionChain [0x77B9B74B+107]\n\tRtlClearBits [0x77B9B6CF+191]\n"
     ]
    }
   ],
   "source": [
    "#Step 1. 필요한 모듈을 로딩합니다\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "import time \n",
    "\n",
    "#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.\n",
    "query_txt = '인공지능'\n",
    "\n",
    "#Step 3. 수집된 데이터를 저장할 파일 이름 입력받기 \n",
    "f_dir = input(\"2.파일을 저장할 폴더명만 쓰세요(기본값:c:\\\\py_temp\\\\):\")\n",
    "if f_dir == '' :\n",
    "    f_dir=\"c:\\\\py_temp\\\\\"\n",
    "\n",
    "#Step 4. 크롬 드라이버 설정 및 웹 페이지 열기 - 변경\n",
    "#\"c:/py_temp/chromedriver.exe\"\n",
    "driver = webdriver.Chrome()\n",
    "url = 'http://www.riss.kr/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 5. 자동으로 검색어 입력 후 조회하기 (class가 쿼리인 것)\n",
    "driver.find_element(By.ID,'query').send_keys(query_txt+'\\n')\n",
    "\n",
    "#Step 6.학위 논문 선택하기\n",
    "driver.find_element(By.LINK_TEXT,'단행본').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Step 7.Beautiful Soup 로 본문 내용만 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "soup_1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "#Step 8. 총 검색 건수를 보여주고 수집할 건수 입력받기 - 벼경\n",
    "import math\n",
    "total_cnt = soup_1.find('div','searchBox pd').find('span','num').get_text() #논문에 저장되어 있는 총 건수 \n",
    "print('검색하신 키워드 %s (으)로 총 %s 건의 학위논문이 검색되었습니다' %(query_txt,total_cnt))\n",
    "cnt = 15\n",
    "page_cnt = math.ceil(cnt / 10) # 1페이지에 10건씩 저장되니까, 총 몇장을 넘겨야지 내가 원하는 만큼 수집할 수 있을지 받는다. \n",
    "print('%s 건의 데이터를 수집하기 위해 %s 페이지의 게시물을 조회합니다.' %(cnt,page_cnt))\n",
    "print(\"\\n\")\n",
    "\n",
    "#Step 9. 데이터 수집하기\n",
    "no2=[]           # 게시글 번호 컬럼\n",
    "title2=[ ]       # 게시글 제목 컬럼\n",
    "author2=[]       # 논문 저자 컬럼\n",
    "company2=[ ]     # 소속 기관 컬럼\n",
    "date2=[ ]        # 게시글 날짜 컬럼\n",
    "suksa2=[ ]       # 국내석사 컬럼\n",
    "contents2=[]     # 초록내용\n",
    "full_url2=[]     # 논문 원본 URL\n",
    "\n",
    "no = 1           # 게시글 번호 초기값\n",
    "            \n",
    "for a in range(1,page_cnt+1) :\n",
    "    print(\"\\n\")\n",
    "    print(\"%s 페이지 내용 수집 시작합니다 =======================\" %a)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    content_list = soup.find('div','srchResultListW').find_all('li')\n",
    "\n",
    "    for i in content_list:\n",
    "        # 논문 제목 체크하기\n",
    "        # try 후 else는 무조건 출력한다. \n",
    "        try:\n",
    "            title=i.find('p','title').get_text().strip()\n",
    "        except :\n",
    "            continue \n",
    "        else :\n",
    "            # 1.게시글 번호\n",
    "            print(\"\\n\")\n",
    "            print(\"%s 번째 정보를 추출하고 있습니다============\" %no)\n",
    "            no2.append(no)\n",
    "            print(\"1.번호 : %s\" %no)\n",
    "            \n",
    "            # 2. 논문 제목\n",
    "            title2.append(title.strip())\n",
    "            print(\"2.제목 : %s\" %title.strip())\n",
    "\n",
    "            # 3. 작성자\n",
    "            try :\n",
    "                author=i.find('p','etc').find('span','writer').get_text().strip()\n",
    "            except :\n",
    "                author = '작성자가 없습니다'\n",
    "                print(\"3.작성자 : %s\" %author.strip()) #공백제거\n",
    "                author2.append(author.strip())\n",
    "            else :\n",
    "                author2.append(author.strip())\n",
    "                print(\"3.작성자 : %s\" %author.strip())\n",
    "\n",
    "            # 4. 소속기관\n",
    "            try :\n",
    "                company=i.find('p','etc').find('span','assigned').get_text().strip()\n",
    "            except :\n",
    "                company='소속 기관이 없습니다'\n",
    "                company2.append(company.strip())\n",
    "                print(\"4.소속기관 : %s\" %company.strip())\n",
    "            else :\n",
    "                company2.append(company.strip())\n",
    "                print(\"4.소속기관 : %s\" %company.strip())\n",
    "\n",
    "            # 5. 발표날짜\n",
    "            try :\n",
    "                #p의 span 요소를 date_1에 저장하는데 그 중 발표날짜는 인덱스 2에 저장되어 있다. \n",
    "                date_1 =i.find('p','etc').find_all('span')\n",
    "                date_2 = date_1[2].get_text().strip()\n",
    "            except :\n",
    "                date_2='발표날짜가 없습니다'\n",
    "                date2.append(date_2)\n",
    "                print(\"5.발표년도 : %s\" %date_2)\n",
    "            else :\n",
    "                date2.append(date_2)\n",
    "                print(\"5.발표년도 : %s\" %date_2)\n",
    "\n",
    "            # 6.학위여부\n",
    "            try :\n",
    "                #p의 span 요소를 suksa_1에 저장하는데 그 중 발표날짜는 인덱스 3에 저장되어 있다. \n",
    "                suksa_1 =i.find('p','etc').find_all('span')\n",
    "                suksa_2 = suksa_1[3].get_text().strip()\n",
    "            except :\n",
    "                suksa_2='학위가 없습니다'\n",
    "                suksa2.append(suksa_2)\n",
    "                print(\"6.학위여부 : %s\" %suksa_2)\n",
    "            else :\n",
    "                suksa2.append(suksa_2)\n",
    "                print(\"6.학위여부 : %s\" %suksa_2)\n",
    "\n",
    "            # 7.초록 내용-해당 논문의 상세 내역에서 추출할 수 있음.    \n",
    "            url_1 = i.find('p','title').find('a')['href']\n",
    "            full_url = 'http://www.riss.kr'+url_1\n",
    "            time.sleep(1)\n",
    "            driver.get(full_url) # 웹페이지를 굳이 열어야하는가..?\n",
    "\n",
    "            soup_1 = BeautifulSoup(driver.page_source, 'html.parser')  \n",
    "            try :\n",
    "                cont=soup_1.find(\"div\",\"text\").find('p').get_text().replace(\"\\n\",\"\").strip()\n",
    "            except :\n",
    "                cont='초록이 없습니다'\n",
    "                contents2.append(cont)\n",
    "                print(\"7.초록내용 : %s\" %cont)\n",
    "            else :\n",
    "                contents2.append(cont)\n",
    "                print(\"7.초록내용 : %s\" %cont)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 8.논문 url 주소\n",
    "            full_url2.append(full_url)\n",
    "            print('8.논문 URL 주소:' , full_url)\n",
    "\n",
    "            driver.back()  # 이전 페이지로 돌아가기\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            no += 1\n",
    "            \n",
    "            if no > cnt :\n",
    "                break \n",
    "                            \n",
    "    a += 1 \n",
    "    b = str(a)\n",
    "\n",
    "    try :\n",
    "        driver.find_element(By.LINK_TEXT ,'%s' %b).click() \n",
    "    except :\n",
    "        driver.find_element(By.LINK_TEXT,'다음 페이지로').click()\n",
    "        \n",
    "print(\"요청하신 작업이 모두 완료되었습니다\")\n",
    "\n",
    "# Step 10. 수집된 데이터를 xls와 csv 형태로 저장하기\n",
    "# 현재 날짜와 시간으로 폴더 만들고 파일 이름 설정하기\n",
    "import os\n",
    "\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' %(n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)\n",
    "\n",
    "os.makedirs(f_dir+'RISS'+'-'+s+'-'+'학위논문') #지정된 디렉토리를 생성한다. \n",
    "\n",
    "fc_name = f_dir+'RISS'+'-'+s+'-'+'학위논문'+'\\\\'+'RISS'+'-'+s+'-'+'학위논문'+'.csv'\n",
    "fx_name = f_dir+'RISS'+'-'+s+'-'+'학위논문'+'\\\\'+'RISS'+'-'+s+'-'+'학위논문'+'.xls'\n",
    "\n",
    "# 데이터 프레임 생성 후 xls , csv 형식으로 저장하기\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['번호']=pd.Series(no2)\n",
    "df['제목']=pd.Series(title2)\n",
    "df['저자']=pd.Series(author2)\n",
    "df['소속(발행)기관']=pd.Series(company2)\n",
    "df['날짜']=pd.Series(date2)\n",
    "df['학위(논문일경우)']=pd.Series(suksa2)\n",
    "df['초록(논문일경우)']=pd.Series(contents2)\n",
    "df['자료URL주소']=pd.Series(full_url2)\n",
    "\n",
    "# xls 형태로 저장하기\n",
    "df.to_excel(fx_name,index=False, encoding=\"utf-8\" , engine='openpyxl')\n",
    "#df.to_excel(fx_name,index=False , engine='openpyxl') - 가능\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "df.to_csv(fc_name,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
